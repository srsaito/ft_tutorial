{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch==2.4.0 in /home/ubuntu/.local/lib/python3.10/site-packages (2.4.0)\n",
      "Requirement already satisfied: tensorboard in /home/ubuntu/.local/lib/python3.10/site-packages (2.18.0)\n",
      "Requirement already satisfied: filelock in /home/ubuntu/.local/lib/python3.10/site-packages (from torch==2.4.0) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch==2.4.0) (4.12.2)\n",
      "Requirement already satisfied: sympy in /home/ubuntu/.local/lib/python3.10/site-packages (from torch==2.4.0) (1.13.1)\n",
      "Requirement already satisfied: networkx in /home/ubuntu/.local/lib/python3.10/site-packages (from torch==2.4.0) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch==2.4.0) (3.1.5)\n",
      "Requirement already satisfied: fsspec in /home/ubuntu/.local/lib/python3.10/site-packages (from torch==2.4.0) (2023.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch==2.4.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch==2.4.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch==2.4.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch==2.4.0) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch==2.4.0) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch==2.4.0) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch==2.4.0) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch==2.4.0) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch==2.4.0) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch==2.4.0) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch==2.4.0) (12.1.105)\n",
      "Requirement already satisfied: triton==3.0.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch==2.4.0) (3.0.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/ubuntu/.local/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.4.0) (12.4.127)\n",
      "Requirement already satisfied: absl-py>=0.4 in /home/ubuntu/.local/lib/python3.10/site-packages (from tensorboard) (2.1.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /home/ubuntu/.local/lib/python3.10/site-packages (from tensorboard) (1.70.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/ubuntu/.local/lib/python3.10/site-packages (from tensorboard) (3.7)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from tensorboard) (1.24.3)\n",
      "Requirement already satisfied: packaging in /home/ubuntu/.local/lib/python3.10/site-packages (from tensorboard) (24.2)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /home/ubuntu/.local/lib/python3.10/site-packages (from tensorboard) (5.29.3)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/lib/python3/dist-packages (from tensorboard) (59.6.0)\n",
      "Requirement already satisfied: six>1.9 in /usr/lib/python3/dist-packages (from tensorboard) (1.16.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from tensorboard) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from tensorboard) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from sympy->torch==2.4.0) (1.3.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers==4.44.2 in /home/ubuntu/.local/lib/python3.10/site-packages (4.44.2)\n",
      "Requirement already satisfied: datasets==2.21.0 in /home/ubuntu/.local/lib/python3.10/site-packages (2.21.0)\n",
      "Requirement already satisfied: accelerate==0.33.0 in /home/ubuntu/.local/lib/python3.10/site-packages (0.33.0)\n",
      "Requirement already satisfied: evaluate==0.4.2 in /home/ubuntu/.local/lib/python3.10/site-packages (0.4.2)\n",
      "Requirement already satisfied: bitsandbytes==0.43.3 in /home/ubuntu/.local/lib/python3.10/site-packages (0.43.3)\n",
      "Requirement already satisfied: trl==0.9.6 in /home/ubuntu/.local/lib/python3.10/site-packages (0.9.6)\n",
      "Requirement already satisfied: peft==0.12.0 in /home/ubuntu/.local/lib/python3.10/site-packages (0.12.0)\n",
      "Requirement already satisfied: filelock in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers==4.44.2) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers==4.44.2) (0.28.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers==4.44.2) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers==4.44.2) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from transformers==4.44.2) (5.4.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers==4.44.2) (2024.11.6)\n",
      "Requirement already satisfied: requests in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers==4.44.2) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers==4.44.2) (0.5.2)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers==4.44.2) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers==4.44.2) (4.67.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from datasets==2.21.0) (19.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from datasets==2.21.0) (0.3.7)\n",
      "Requirement already satisfied: pandas in /home/ubuntu/.local/lib/python3.10/site-packages (from datasets==2.21.0) (2.2.3)\n",
      "Requirement already satisfied: xxhash in /home/ubuntu/.local/lib/python3.10/site-packages (from datasets==2.21.0) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /home/ubuntu/.local/lib/python3.10/site-packages (from datasets==2.21.0) (0.70.15)\n",
      "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets==2.21.0) (2023.10.0)\n",
      "Requirement already satisfied: aiohttp in /home/ubuntu/.local/lib/python3.10/site-packages (from datasets==2.21.0) (3.11.12)\n",
      "Requirement already satisfied: psutil in /home/ubuntu/.local/lib/python3.10/site-packages (from accelerate==0.33.0) (6.1.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from accelerate==0.33.0) (2.4.0)\n",
      "Requirement already satisfied: tyro>=0.5.11 in /home/ubuntu/.local/lib/python3.10/site-packages (from trl==0.9.6) (0.9.13)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from aiohttp->datasets==2.21.0) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/ubuntu/.local/lib/python3.10/site-packages (from aiohttp->datasets==2.21.0) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from aiohttp->datasets==2.21.0) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from aiohttp->datasets==2.21.0) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from aiohttp->datasets==2.21.0) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ubuntu/.local/lib/python3.10/site-packages (from aiohttp->datasets==2.21.0) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from aiohttp->datasets==2.21.0) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from aiohttp->datasets==2.21.0) (1.18.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ubuntu/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.44.2) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ubuntu/.local/lib/python3.10/site-packages (from requests->transformers==4.44.2) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers==4.44.2) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests->transformers==4.44.2) (1.26.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->transformers==4.44.2) (2020.6.20)\n",
      "Requirement already satisfied: sympy in /home/ubuntu/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.33.0) (1.13.1)\n",
      "Requirement already satisfied: networkx in /home/ubuntu/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.33.0) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.33.0) (3.1.5)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.33.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.33.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.33.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.33.0) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.33.0) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.33.0) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.33.0) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.33.0) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.33.0) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.33.0) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.33.0) (12.1.105)\n",
      "Requirement already satisfied: triton==3.0.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.33.0) (3.0.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/ubuntu/.local/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate==0.33.0) (12.4.127)\n",
      "Requirement already satisfied: docstring-parser>=0.15 in /home/ubuntu/.local/lib/python3.10/site-packages (from tyro>=0.5.11->trl==0.9.6) (0.16)\n",
      "Requirement already satisfied: rich>=11.1.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from tyro>=0.5.11->trl==0.9.6) (13.9.4)\n",
      "Requirement already satisfied: shtab>=1.5.6 in /home/ubuntu/.local/lib/python3.10/site-packages (from tyro>=0.5.11->trl==0.9.6) (1.7.1)\n",
      "Requirement already satisfied: typeguard>=4.0.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from tyro>=0.5.11->trl==0.9.6) (4.4.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/ubuntu/.local/lib/python3.10/site-packages (from pandas->datasets==2.21.0) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas->datasets==2.21.0) (2022.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/ubuntu/.local/lib/python3.10/site-packages (from pandas->datasets==2.21.0) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets==2.21.0) (1.16.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.9.6) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.9.6) (2.19.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate==0.33.0) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate==0.33.0) (1.3.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl==0.9.6) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "# https://www.philschmid.de/fine-tune-llms-in-2024-with-trl\n",
    "\n",
    "# Install Pytorch & other libraries\n",
    "# !pip install \"torch==2.1.2\" tensorboard\n",
    "# Upgrade to latest stable PyTorch\n",
    "!pip install torch==2.4.0 tensorboard\n",
    "\n",
    "# Install Hugging Face libraries\n",
    "!pip install  --upgrade \\\n",
    "  \"transformers==4.44.2\" \\\n",
    "  \"datasets==2.21.0\" \\\n",
    "  \"accelerate==0.33.0\" \\\n",
    "  \"evaluate==0.4.2\" \\\n",
    "  \"bitsandbytes==0.43.3\" \\\n",
    "  \"trl==0.9.6\" \\\n",
    "  \"peft==0.12.0\"\n",
    "\n",
    "# install peft & trl from github\n",
    "# !pip install git+https://github.com/huggingface/trl@a3c5b7178ac4f65569975efadc97db2f3749c65e --upgrade\n",
    "# !pip install git+https://github.com/huggingface/peft@4a1559582281fc3c9283892caea8ccef1d6f5a4f --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA A10G\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-02-10 22:33:20--  https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-ubuntu2204.pin\n",
      "Resolving developer.download.nvidia.com (developer.download.nvidia.com)... 23.55.102.137\n",
      "Connecting to developer.download.nvidia.com (developer.download.nvidia.com)|23.55.102.137|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 190 [application/octet-stream]\n",
      "Saving to: ‘cuda-ubuntu2204.pin’\n",
      "\n",
      "cuda-ubuntu2204.pin 100%[===================>]     190  --.-KB/s    in 0s      \n",
      "\n",
      "2025-02-10 22:33:20 (266 MB/s) - ‘cuda-ubuntu2204.pin’ saved [190/190]\n",
      "\n",
      "Warning: apt-key is deprecated. Manage keyring files in trusted.gpg.d instead (see apt-key(8)).\n",
      "Executing: /tmp/apt-key-gpghome.jcda0O8Lko/gpg.1.sh --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/3bf863cc.pub\n",
      "gpg: requesting key from 'https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/3bf863cc.pub'\n",
      "gpg: key A4B469963BF863CC: \"cudatools <cudatools@nvidia.com>\" not changed\n",
      "gpg: Total number processed: 1\n",
      "gpg:              unchanged: 1\n",
      "Repository: 'deb https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/ /'\n",
      "Description:\n",
      "Archive for codename: / components: \n",
      "More info: https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/\n",
      "Adding repository.\n",
      "Found existing deb entry in /etc/apt/sources.list.d/archive_uri-https_developer_download_nvidia_com_compute_cuda_repos_ubuntu2204_x86_64_-jammy.list\n",
      "Adding deb entry to /etc/apt/sources.list.d/archive_uri-https_developer_download_nvidia_com_compute_cuda_repos_ubuntu2204_x86_64_-jammy.list\n",
      "Found existing deb-src entry in /etc/apt/sources.list.d/archive_uri-https_developer_download_nvidia_com_compute_cuda_repos_ubuntu2204_x86_64_-jammy.list\n",
      "Adding disabled deb-src entry to /etc/apt/sources.list.d/archive_uri-https_developer_download_nvidia_com_compute_cuda_repos_ubuntu2204_x86_64_-jammy.list\n",
      "Hit:1 http://us-west-2.ec2.archive.ubuntu.com/ubuntu jammy InRelease\n",
      "Get:2 http://us-west-2.ec2.archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
      "Hit:3 http://us-west-2.ec2.archive.ubuntu.com/ubuntu jammy-backports InRelease \n",
      "Hit:4 https://download.docker.com/linux/ubuntu jammy InRelease                 \n",
      "Get:5 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]      \n",
      "Hit:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
      "Get:7 http://us-west-2.ec2.archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2315 kB]\n",
      "Get:8 http://us-west-2.ec2.archive.ubuntu.com/ubuntu jammy-updates/main i386 Packages [752 kB]\n",
      "Get:9 http://us-west-2.ec2.archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1185 kB]\n",
      "Get:10 http://us-west-2.ec2.archive.ubuntu.com/ubuntu jammy-updates/universe i386 Packages [757 kB]\n",
      "Fetched 5266 kB in 1s (6793 kB/s)                                              \n",
      "Reading package lists... Done\n",
      "W: https://download.docker.com/linux/ubuntu/dists/jammy/InRelease: Key is stored in legacy trusted.gpg keyring (/etc/apt/trusted.gpg), see the DEPRECATION section in apt-key(8) for details.\n",
      "W: Target Packages (Packages) is configured multiple times in /etc/apt/sources.list.d/archive_uri-https_developer_download_nvidia_com_compute_cuda_repos_ubuntu2204_x86_64_-jammy.list:1 and /etc/apt/sources.list.d/cuda-ubuntu2204-x86_64.list:1\n",
      "W: Target Translations (en) is configured multiple times in /etc/apt/sources.list.d/archive_uri-https_developer_download_nvidia_com_compute_cuda_repos_ubuntu2204_x86_64_-jammy.list:1 and /etc/apt/sources.list.d/cuda-ubuntu2204-x86_64.list:1\n",
      "W: Target Packages (Packages) is configured multiple times in /etc/apt/sources.list.d/archive_uri-https_developer_download_nvidia_com_compute_cuda_repos_ubuntu2204_x86_64_-jammy.list:1 and /etc/apt/sources.list.d/cuda-ubuntu2204-x86_64.list:1\n",
      "W: Target Translations (en) is configured multiple times in /etc/apt/sources.list.d/archive_uri-https_developer_download_nvidia_com_compute_cuda_repos_ubuntu2204_x86_64_-jammy.list:1 and /etc/apt/sources.list.d/cuda-ubuntu2204-x86_64.list:1\n",
      "Hit:1 http://us-west-2.ec2.archive.ubuntu.com/ubuntu jammy InRelease\n",
      "Hit:2 http://us-west-2.ec2.archive.ubuntu.com/ubuntu jammy-updates InRelease   \u001b[0m\n",
      "Hit:3 http://us-west-2.ec2.archive.ubuntu.com/ubuntu jammy-backports InRelease \n",
      "Hit:4 https://download.docker.com/linux/ubuntu jammy InRelease                 \u001b[0m\n",
      "Hit:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
      "Hit:6 http://security.ubuntu.com/ubuntu jammy-security InRelease             \n",
      "Reading package lists... Done\u001b[33m\u001b[33m\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "8 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
      "\u001b[1;33mW: \u001b[0mhttps://download.docker.com/linux/ubuntu/dists/jammy/InRelease: Key is stored in legacy trusted.gpg keyring (/etc/apt/trusted.gpg), see the DEPRECATION section in apt-key(8) for details.\u001b[0m\n",
      "\u001b[1;33mW: \u001b[0mTarget Packages (Packages) is configured multiple times in /etc/apt/sources.list.d/archive_uri-https_developer_download_nvidia_com_compute_cuda_repos_ubuntu2204_x86_64_-jammy.list:1 and /etc/apt/sources.list.d/cuda-ubuntu2204-x86_64.list:1\u001b[0m\n",
      "\u001b[1;33mW: \u001b[0mTarget Translations (en) is configured multiple times in /etc/apt/sources.list.d/archive_uri-https_developer_download_nvidia_com_compute_cuda_repos_ubuntu2204_x86_64_-jammy.list:1 and /etc/apt/sources.list.d/cuda-ubuntu2204-x86_64.list:1\u001b[0m\n",
      "\u001b[1;33mW: \u001b[0mTarget Packages (Packages) is configured multiple times in /etc/apt/sources.list.d/archive_uri-https_developer_download_nvidia_com_compute_cuda_repos_ubuntu2204_x86_64_-jammy.list:1 and /etc/apt/sources.list.d/cuda-ubuntu2204-x86_64.list:1\u001b[0m\n",
      "\u001b[1;33mW: \u001b[0mTarget Translations (en) is configured multiple times in /etc/apt/sources.list.d/archive_uri-https_developer_download_nvidia_com_compute_cuda_repos_ubuntu2204_x86_64_-jammy.list:1 and /etc/apt/sources.list.d/cuda-ubuntu2204-x86_64.list:1\u001b[0m\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "cuda-12-1 is already the newest version (12.1.1-1).\n",
      "The following packages were automatically installed and are no longer required:\n",
      "  cuda-cccl-11-7 cuda-command-line-tools-11-7 cuda-compiler-11-7\n",
      "  cuda-cudart-11-7 cuda-cudart-dev-11-7 cuda-cuobjdump-11-7 cuda-cupti-11-7\n",
      "  cuda-cupti-dev-11-7 cuda-cuxxfilt-11-7 cuda-demo-suite-11-7\n",
      "  cuda-documentation-11-7 cuda-driver-dev-11-7 cuda-gdb-11-7\n",
      "  cuda-libraries-11-7 cuda-libraries-dev-11-7 cuda-memcheck-11-7\n",
      "  cuda-nsight-11-7 cuda-nsight-compute-11-7 cuda-nsight-systems-11-7\n",
      "  cuda-nvcc-11-7 cuda-nvdisasm-11-7 cuda-nvml-dev-11-7 cuda-nvprof-11-7\n",
      "  cuda-nvprune-11-7 cuda-nvrtc-11-7 cuda-nvrtc-dev-11-7 cuda-nvtx-11-7\n",
      "  cuda-nvvp-11-7 cuda-runtime-11-7 cuda-sanitizer-11-7 cuda-toolkit-11-7\n",
      "  cuda-toolkit-11-7-config-common cuda-toolkit-11-config-common\n",
      "  cuda-tools-11-7 cuda-visual-tools-11-7 gds-tools-11-7 libaccinj64-11.5\n",
      "  libasyncns0 libbabeltrace1 libboost-regex1.74.0 libcub-dev libcublas-11-7\n",
      "  libcublas-dev-11-7 libcublas11 libcublaslt11 libcudart11.0 libcufft-11-7\n",
      "  libcufft-dev-11-7 libcufft10 libcufftw10 libcufile-11-7 libcufile-dev-11-7\n",
      "  libcuinj64-11.5 libcupti-dev libcupti-doc libcupti11.5 libcurand-11-7\n",
      "  libcurand-dev-11-7 libcurand10 libcusolver-11-7 libcusolver-dev-11-7\n",
      "  libcusolver11 libcusolvermg11 libcusparse-11-7 libcusparse-dev-11-7\n",
      "  libcusparse11 libdebuginfod-common libdebuginfod1 libegl-dev libflac8\n",
      "  libgl-dev libgl1-mesa-dev libgles-dev libgles1 libgles2 libglvnd-core-dev\n",
      "  libglvnd-dev libglx-dev libipt2 libnpp-11-7 libnpp-dev-11-7 libnppc11\n",
      "  libnppial11 libnppicc11 libnppidei11 libnppif11 libnppig11 libnppim11\n",
      "  libnppist11 libnppisu11 libnppitc11 libnpps11 libnvblas11 libnvidia-ml-dev\n",
      "  libnvjpeg-11-7 libnvjpeg-dev-11-7 libnvjpeg11 libnvrtc-builtins11.5\n",
      "  libnvrtc11.2 libnvtoolsext1 libnvvm4 libogg0 libopengl-dev libopus0\n",
      "  libpthread-stubs0-dev libpulse0 libsndfile1 libsource-highlight-common\n",
      "  libsource-highlight4v5 libtbb-dev libtbb12 libtbbmalloc2 libthrust-dev\n",
      "  libvdpau-dev libvorbis0a libvorbisenc2 libx11-dev libxau-dev libxcb1-dev\n",
      "  libxdmcp-dev node-html5shiv nsight-compute-2022.2.1 nsight-systems-2022.1.3\n",
      "  nvidia-cuda-dev nvidia-cuda-gdb nvidia-cuda-toolkit-doc nvidia-opencl-dev\n",
      "  nvidia-profiler nvidia-visual-profiler ocl-icd-libopencl1 ocl-icd-opencl-dev\n",
      "  opencl-c-headers opencl-clhpp-headers openjdk-8-jre x11proto-dev\n",
      "  xorg-sgml-doctools xtrans-dev\n",
      "Use 'sudo apt autoremove' to remove them.\n",
      "0 upgraded, 0 newly installed, 0 to remove and 8 not upgraded.\n",
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2023 NVIDIA Corporation\n",
      "Built on Mon_Apr__3_17:16:06_PDT_2023\n",
      "Cuda compilation tools, release 12.1, V12.1.105\n",
      "Build cuda_12.1.r12.1/compiler.32688072_0\n"
     ]
    }
   ],
   "source": [
    "# Only need to run this cell once\n",
    "\n",
    "# Regular install didn't work here b/c Ubuntu 22.04 version of CUDA toolkit is too old\n",
    "\n",
    "# Update package list\n",
    "# !sudo apt update\n",
    "\n",
    "# Install CUDA toolkit\n",
    "# !sudo apt install -y nvidia-cuda-toolkit\n",
    "\n",
    "# Remove current CUDA installation\n",
    "# !sudo apt remove -y cuda-11-7\n",
    "\n",
    "# Add NVIDIA repository (if not already added)\n",
    "!wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-ubuntu2204.pin\n",
    "!sudo mv cuda-ubuntu2204.pin /etc/apt/preferences.d/cuda-repository-pin-600\n",
    "!sudo apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/3bf863cc.pub\n",
    "!sudo add-apt-repository -y \"deb https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/ /\"\n",
    "\n",
    "# Update package list\n",
    "!sudo apt update\n",
    "\n",
    "# Install CUDA 12.1\n",
    "!sudo apt install -y cuda-12-1\n",
    "\n",
    "# Set environment variables for CUDA 12.1\n",
    "import os\n",
    "os.environ['CUDA_HOME'] = '/usr/local/cuda-12.1'\n",
    "os.environ['PATH'] = f\"/usr/local/cuda-12.1/bin:{os.environ['PATH']}\"\n",
    "os.environ['LD_LIBRARY_PATH'] = f\"/usr/local/cuda-12.1/lib64:{os.environ.get('LD_LIBRARY_PATH', '')}\"\n",
    "\n",
    "# Verify CUDA version\n",
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only need to run once\n",
    "# !echo 'export CUDA_HOME=/usr/local/cuda-12.1' >> ~/.bashrc\n",
    "# !echo 'export PATH=/usr/local/cuda-12.1/bin:$PATH' >> ~/.bashrc\n",
    "# !echo 'export LD_LIBRARY_PATH=/usr/local/cuda-12.1/lib64:$LD_LIBRARY_PATH' >> ~/.bashrc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch; assert torch.cuda.get_device_capability()[0] >= 8, 'Hardware not supported for Flash Attention'\n",
    "\n",
    "# Commenting out because it causes system to hang\n",
    "# install flash-attn\n",
    "# !pip install ninja packaging\n",
    "# !MAX_JOBS=4 pip install flash-attn --no-build-isolation\n",
    "\n",
    "# Undo failed prior flash-attn compilation\n",
    "# !rm -rf ~/.cache/torch_extensions\n",
    "# !pip uninstall flash-attn -y\n",
    "\n",
    "# Commented out because it causes system to hang\n",
    "# Install precompiled version (prebuilt wheel)\n",
    "# !pip install flash-attn --index-url https://flash-attention-builds.s3.us-west-2.amazonaws.com/\n",
    "# !pip install flash-attn --index-url https://pypi.huggingface.co/simple/\n",
    "\n",
    "# Upgrade to latest stable PyTorch\n",
    "# !pip install torch==2.2.0\n",
    "\n",
    "# Can use latest NumPy\n",
    "# !pip install numpy --upgrade  # This will get us back to 2.2.2\n",
    "\n",
    "# !pip install numpy==1.24.3 --force-reinstall\n",
    "\n",
    "# Install flash-attn\n",
    "#!MAX_JOBS=4 pip install flash-attn --no-build-isolation --verbose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping flash-attn as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0mDefaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: ninja in /home/ubuntu/.local/lib/python3.10/site-packages (1.11.1.3)\n",
      "Requirement already satisfied: packaging in /home/ubuntu/.local/lib/python3.10/site-packages (24.2)\n",
      "Using pip 25.0 from /home/ubuntu/.local/lib/python3.10/site-packages/pip (python 3.10)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting flash-attn==2.4.2\n",
      "  Downloading flash_attn-2.4.2.tar.gz (2.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l  Running command python setup.py egg_info\n",
      "  fatal: not a git repository (or any of the parent directories): .git\n",
      "\n",
      "\n",
      "  torch.__version__  = 2.4.0+cu121\n",
      "\n",
      "\n",
      "  running egg_info\n",
      "  creating /tmp/pip-pip-egg-info-_c81r2sr/flash_attn.egg-info\n",
      "  writing /tmp/pip-pip-egg-info-_c81r2sr/flash_attn.egg-info/PKG-INFO\n",
      "  writing dependency_links to /tmp/pip-pip-egg-info-_c81r2sr/flash_attn.egg-info/dependency_links.txt\n",
      "  writing requirements to /tmp/pip-pip-egg-info-_c81r2sr/flash_attn.egg-info/requires.txt\n",
      "  writing top-level names to /tmp/pip-pip-egg-info-_c81r2sr/flash_attn.egg-info/top_level.txt\n",
      "  writing manifest file '/tmp/pip-pip-egg-info-_c81r2sr/flash_attn.egg-info/SOURCES.txt'\n",
      "  reading manifest file '/tmp/pip-pip-egg-info-_c81r2sr/flash_attn.egg-info/SOURCES.txt'\n",
      "  reading manifest template 'MANIFEST.in'\n",
      "  warning: no files found matching '*.cu' under directory 'flash_attn'\n",
      "  warning: no files found matching '*.h' under directory 'flash_attn'\n",
      "  warning: no files found matching '*.cuh' under directory 'flash_attn'\n",
      "  warning: no files found matching '*.cpp' under directory 'flash_attn'\n",
      "  warning: no files found matching '*.hpp' under directory 'flash_attn'\n",
      "  adding license file 'LICENSE'\n",
      "  adding license file 'AUTHORS'\n",
      "  writing manifest file '/tmp/pip-pip-egg-info-_c81r2sr/flash_attn.egg-info/SOURCES.txt'\n",
      "\u001b[?25hdone\n",
      "Requirement already satisfied: einops in /home/ubuntu/.local/lib/python3.10/site-packages (from flash-attn==2.4.2) (0.8.0)\n",
      "Requirement already satisfied: ninja in /home/ubuntu/.local/lib/python3.10/site-packages (from flash-attn==2.4.2) (1.11.1.3)\n",
      "Requirement already satisfied: packaging in /home/ubuntu/.local/lib/python3.10/site-packages (from flash-attn==2.4.2) (24.2)\n",
      "Requirement already satisfied: torch in /home/ubuntu/.local/lib/python3.10/site-packages (from flash-attn==2.4.2) (2.4.0)\n",
      "Requirement already satisfied: filelock in /home/ubuntu/.local/lib/python3.10/site-packages (from torch->flash-attn==2.4.2) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch->flash-attn==2.4.2) (4.12.2)\n",
      "Requirement already satisfied: sympy in /home/ubuntu/.local/lib/python3.10/site-packages (from torch->flash-attn==2.4.2) (1.13.1)\n",
      "Requirement already satisfied: networkx in /home/ubuntu/.local/lib/python3.10/site-packages (from torch->flash-attn==2.4.2) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/lib/python3/dist-packages (from torch->flash-attn==2.4.2) (3.0.3)\n",
      "Requirement already satisfied: fsspec in /home/ubuntu/.local/lib/python3.10/site-packages (from torch->flash-attn==2.4.2) (2023.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch->flash-attn==2.4.2) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch->flash-attn==2.4.2) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch->flash-attn==2.4.2) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch->flash-attn==2.4.2) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch->flash-attn==2.4.2) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch->flash-attn==2.4.2) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch->flash-attn==2.4.2) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch->flash-attn==2.4.2) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch->flash-attn==2.4.2) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch->flash-attn==2.4.2) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch->flash-attn==2.4.2) (12.1.105)\n",
      "Requirement already satisfied: triton==3.0.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch->flash-attn==2.4.2) (3.0.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/ubuntu/.local/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->flash-attn==2.4.2) (12.4.127)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from sympy->torch->flash-attn==2.4.2) (1.3.0)\n",
      "Building wheels for collected packages: flash-attn\n",
      "  Building wheel for flash-attn (setup.py) ... \u001b[?25l  Running command python setup.py bdist_wheel\n",
      "  fatal: not a git repository (or any of the parent directories): .git\n",
      "\n",
      "\n",
      "  torch.__version__  = 2.4.0+cu121\n",
      "\n",
      "\n",
      "  running bdist_wheel\n",
      "  Guessing wheel URL:  https://github.com/Dao-AILab/flash-attention/releases/download/v2.4.2/flash_attn-2.4.2+cu122torch2.4cxx11abiFALSE-cp310-cp310-linux_x86_64.whl\n",
      "  Precompiled wheel not found. Building from source...\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build/lib.linux-x86_64-3.10\n",
      "  creating build/lib.linux-x86_64-3.10/flash_attn\n",
      "  copying flash_attn/flash_blocksparse_attention.py -> build/lib.linux-x86_64-3.10/flash_attn\n",
      "  copying flash_attn/flash_attn_interface.py -> build/lib.linux-x86_64-3.10/flash_attn\n",
      "  copying flash_attn/bert_padding.py -> build/lib.linux-x86_64-3.10/flash_attn\n",
      "  copying flash_attn/flash_attn_triton.py -> build/lib.linux-x86_64-3.10/flash_attn\n",
      "  copying flash_attn/flash_blocksparse_attn_interface.py -> build/lib.linux-x86_64-3.10/flash_attn\n",
      "  copying flash_attn/fused_softmax.py -> build/lib.linux-x86_64-3.10/flash_attn\n",
      "  copying flash_attn/flash_attn_triton_og.py -> build/lib.linux-x86_64-3.10/flash_attn\n",
      "  copying flash_attn/__init__.py -> build/lib.linux-x86_64-3.10/flash_attn\n",
      "  creating build/lib.linux-x86_64-3.10/flash_attn/modules\n",
      "  copying flash_attn/modules/mlp.py -> build/lib.linux-x86_64-3.10/flash_attn/modules\n",
      "  copying flash_attn/modules/embedding.py -> build/lib.linux-x86_64-3.10/flash_attn/modules\n",
      "  copying flash_attn/modules/block.py -> build/lib.linux-x86_64-3.10/flash_attn/modules\n",
      "  copying flash_attn/modules/mha.py -> build/lib.linux-x86_64-3.10/flash_attn/modules\n",
      "  copying flash_attn/modules/__init__.py -> build/lib.linux-x86_64-3.10/flash_attn/modules\n",
      "  creating build/lib.linux-x86_64-3.10/flash_attn/losses\n",
      "  copying flash_attn/losses/cross_entropy.py -> build/lib.linux-x86_64-3.10/flash_attn/losses\n",
      "  copying flash_attn/losses/__init__.py -> build/lib.linux-x86_64-3.10/flash_attn/losses\n",
      "  creating build/lib.linux-x86_64-3.10/flash_attn/utils\n",
      "  copying flash_attn/utils/pretrained.py -> build/lib.linux-x86_64-3.10/flash_attn/utils\n",
      "  copying flash_attn/utils/benchmark.py -> build/lib.linux-x86_64-3.10/flash_attn/utils\n",
      "  copying flash_attn/utils/generation.py -> build/lib.linux-x86_64-3.10/flash_attn/utils\n",
      "  copying flash_attn/utils/distributed.py -> build/lib.linux-x86_64-3.10/flash_attn/utils\n",
      "  copying flash_attn/utils/__init__.py -> build/lib.linux-x86_64-3.10/flash_attn/utils\n",
      "  creating build/lib.linux-x86_64-3.10/flash_attn/ops\n",
      "  copying flash_attn/ops/fused_dense.py -> build/lib.linux-x86_64-3.10/flash_attn/ops\n",
      "  copying flash_attn/ops/activations.py -> build/lib.linux-x86_64-3.10/flash_attn/ops\n",
      "  copying flash_attn/ops/rms_norm.py -> build/lib.linux-x86_64-3.10/flash_attn/ops\n",
      "  copying flash_attn/ops/layer_norm.py -> build/lib.linux-x86_64-3.10/flash_attn/ops\n",
      "  copying flash_attn/ops/__init__.py -> build/lib.linux-x86_64-3.10/flash_attn/ops\n",
      "  creating build/lib.linux-x86_64-3.10/flash_attn/layers\n",
      "  copying flash_attn/layers/patch_embed.py -> build/lib.linux-x86_64-3.10/flash_attn/layers\n",
      "  copying flash_attn/layers/rotary.py -> build/lib.linux-x86_64-3.10/flash_attn/layers\n",
      "  copying flash_attn/layers/__init__.py -> build/lib.linux-x86_64-3.10/flash_attn/layers\n",
      "  creating build/lib.linux-x86_64-3.10/flash_attn/models\n",
      "  copying flash_attn/models/gptj.py -> build/lib.linux-x86_64-3.10/flash_attn/models\n",
      "  copying flash_attn/models/falcon.py -> build/lib.linux-x86_64-3.10/flash_attn/models\n",
      "  copying flash_attn/models/gpt_neox.py -> build/lib.linux-x86_64-3.10/flash_attn/models\n",
      "  copying flash_attn/models/llama.py -> build/lib.linux-x86_64-3.10/flash_attn/models\n",
      "  copying flash_attn/models/bert.py -> build/lib.linux-x86_64-3.10/flash_attn/models\n",
      "  copying flash_attn/models/gpt.py -> build/lib.linux-x86_64-3.10/flash_attn/models\n",
      "  copying flash_attn/models/vit.py -> build/lib.linux-x86_64-3.10/flash_attn/models\n",
      "  copying flash_attn/models/baichuan.py -> build/lib.linux-x86_64-3.10/flash_attn/models\n",
      "  copying flash_attn/models/btlm.py -> build/lib.linux-x86_64-3.10/flash_attn/models\n",
      "  copying flash_attn/models/opt.py -> build/lib.linux-x86_64-3.10/flash_attn/models\n",
      "  copying flash_attn/models/bigcode.py -> build/lib.linux-x86_64-3.10/flash_attn/models\n",
      "  copying flash_attn/models/__init__.py -> build/lib.linux-x86_64-3.10/flash_attn/models\n",
      "  creating build/lib.linux-x86_64-3.10/flash_attn/ops/triton\n",
      "  copying flash_attn/ops/triton/linear.py -> build/lib.linux-x86_64-3.10/flash_attn/ops/triton\n",
      "  copying flash_attn/ops/triton/cross_entropy.py -> build/lib.linux-x86_64-3.10/flash_attn/ops/triton\n",
      "  copying flash_attn/ops/triton/rotary.py -> build/lib.linux-x86_64-3.10/flash_attn/ops/triton\n",
      "  copying flash_attn/ops/triton/mlp.py -> build/lib.linux-x86_64-3.10/flash_attn/ops/triton\n",
      "  copying flash_attn/ops/triton/layernorm.py -> build/lib.linux-x86_64-3.10/flash_attn/ops/triton\n",
      "  copying flash_attn/ops/triton/k_activations.py -> build/lib.linux-x86_64-3.10/flash_attn/ops/triton\n",
      "  copying flash_attn/ops/triton/__init__.py -> build/lib.linux-x86_64-3.10/flash_attn/ops/triton\n",
      "  running build_ext\n",
      "  /home/ubuntu/.local/lib/python3.10/site-packages/torch/utils/cpp_extension.py:424: UserWarning: There are no x86_64-linux-gnu-g++ version bounds defined for CUDA version 12.1\n",
      "    warnings.warn(f'There are no {compiler_name} version bounds defined for CUDA version {cuda_str_version}')\n",
      "  building 'flash_attn_2_cuda' extension\n",
      "  creating /tmp/pip-install-xwrdzojx/flash-attn_6c209a798d30416590e3a675176214d5/build/temp.linux-x86_64-3.10\n",
      "  creating /tmp/pip-install-xwrdzojx/flash-attn_6c209a798d30416590e3a675176214d5/build/temp.linux-x86_64-3.10/csrc\n",
      "  creating /tmp/pip-install-xwrdzojx/flash-attn_6c209a798d30416590e3a675176214d5/build/temp.linux-x86_64-3.10/csrc/flash_attn\n",
      "  creating /tmp/pip-install-xwrdzojx/flash-attn_6c209a798d30416590e3a675176214d5/build/temp.linux-x86_64-3.10/csrc/flash_attn/src\n",
      "  Emitting ninja build file /tmp/pip-install-xwrdzojx/flash-attn_6c209a798d30416590e3a675176214d5/build/temp.linux-x86_64-3.10/build.ninja...\n",
      "  Compiling objects...\n",
      "  Using envvar MAX_JOBS (2) as the number of workers...\n",
      "  [1/49] c++ -MMD -MF /tmp/pip-install-xwrdzojx/flash-attn_6c209a798d30416590e3a675176214d5/build/temp.linux-x86_64-3.10/csrc/flash_attn/flash_api.o.d -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/tmp/pip-install-xwrdzojx/flash-attn_6c209a798d30416590e3a675176214d5/csrc/flash_attn -I/tmp/pip-install-xwrdzojx/flash-attn_6c209a798d30416590e3a675176214d5/csrc/flash_attn/src -I/tmp/pip-install-xwrdzojx/flash-attn_6c209a798d30416590e3a675176214d5/csrc/cutlass/include -I/home/ubuntu/.local/lib/python3.10/site-packages/torch/include -I/home/ubuntu/.local/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/home/ubuntu/.local/lib/python3.10/site-packages/torch/include/TH -I/home/ubuntu/.local/lib/python3.10/site-packages/torch/include/THC -I/usr/local/cuda-12.1/include -I/usr/include/python3.10 -c -c /tmp/pip-install-xwrdzojx/flash-attn_6c209a798d30416590e3a675176214d5/csrc/flash_attn/flash_api.cpp -o /tmp/pip-install-xwrdzojx/flash-attn_6c209a798d30416590e3a675176214d5/build/temp.linux-x86_64-3.10/csrc/flash_attn/flash_api.o -O3 -std=c++17 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=flash_attn_2_cuda -D_GLIBCXX_USE_CXX11_ABI=0\n",
      "  /tmp/pip-install-xwrdzojx/flash-attn_6c209a798d30416590e3a675176214d5/csrc/flash_attn/flash_api.cpp: In function ‘void set_params_fprop(Flash_fwd_params&, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, at::Tensor, at::Tensor, at::Tensor, at::Tensor, void*, void*, void*, void*, void*, float, float, int, int)’:\n",
      "  /tmp/pip-install-xwrdzojx/flash-attn_6c209a798d30416590e3a675176214d5/csrc/flash_attn/flash_api.cpp:48:11: warning: ‘void* memset(void*, int, size_t)’ clearing an object of non-trivial type ‘struct Flash_fwd_params’; use assignment or value-initialization instead [-Wclass-memaccess]\n",
      "     48 |     memset(&params, 0, sizeof(params));\n",
      "        |     ~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  In file included from /tmp/pip-install-xwrdzojx/flash-attn_6c209a798d30416590e3a675176214d5/csrc/flash_attn/flash_api.cpp:13:\n",
      "  /tmp/pip-install-xwrdzojx/flash-attn_6c209a798d30416590e3a675176214d5/csrc/flash_attn/src/flash.h:51:8: note: ‘struct Flash_fwd_params’ declared here\n",
      "     51 | struct Flash_fwd_params : public Qkv_params {\n",
      "        |        ^~~~~~~~~~~~~~~~\n",
      "  [2/49] /usr/local/cuda-12.1/bin/nvcc --generate-dependencies-with-compile --dependency-output /tmp/pip-install-xwrdzojx/flash-attn_6c209a798d30416590e3a675176214d5/build/temp.linux-x86_64-3.10/csrc/flash_attn/src/flash_bwd_hdim128_bf16_sm80.o.d -I/tmp/pip-install-xwrdzojx/flash-attn_6c209a798d30416590e3a675176214d5/csrc/flash_attn -I/tmp/pip-install-xwrdzojx/flash-attn_6c209a798d30416590e3a675176214d5/csrc/flash_attn/src -I/tmp/pip-install-xwrdzojx/flash-attn_6c209a798d30416590e3a675176214d5/csrc/cutlass/include -I/home/ubuntu/.local/lib/python3.10/site-packages/torch/include -I/home/ubuntu/.local/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/home/ubuntu/.local/lib/python3.10/site-packages/torch/include/TH -I/home/ubuntu/.local/lib/python3.10/site-packages/torch/include/THC -I/usr/local/cuda-12.1/include -I/usr/include/python3.10 -c -c /tmp/pip-install-xwrdzojx/flash-attn_6c209a798d30416590e3a675176214d5/csrc/flash_attn/src/flash_bwd_hdim128_bf16_sm80.cu -o /tmp/pip-install-xwrdzojx/flash-attn_6c209a798d30416590e3a675176214d5/build/temp.linux-x86_64-3.10/csrc/flash_attn/src/flash_bwd_hdim128_bf16_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math -gencode arch=compute_80,code=sm_80 -gencode arch=compute_90,code=sm_90 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=flash_attn_2_cuda -D_GLIBCXX_USE_CXX11_ABI=0\n",
      "  [3/49] /usr/local/cuda-12.1/bin/nvcc --generate-dependencies-with-compile --dependency-output /tmp/pip-install-xwrdzojx/flash-attn_6c209a798d30416590e3a675176214d5/build/temp.linux-x86_64-3.10/csrc/flash_attn/src/flash_bwd_hdim128_fp16_sm80.o.d -I/tmp/pip-install-xwrdzojx/flash-attn_6c209a798d30416590e3a675176214d5/csrc/flash_attn -I/tmp/pip-install-xwrdzojx/flash-attn_6c209a798d30416590e3a675176214d5/csrc/flash_attn/src -I/tmp/pip-install-xwrdzojx/flash-attn_6c209a798d30416590e3a675176214d5/csrc/cutlass/include -I/home/ubuntu/.local/lib/python3.10/site-packages/torch/include -I/home/ubuntu/.local/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/home/ubuntu/.local/lib/python3.10/site-packages/torch/include/TH -I/home/ubuntu/.local/lib/python3.10/site-packages/torch/include/THC -I/usr/local/cuda-12.1/include -I/usr/include/python3.10 -c -c /tmp/pip-install-xwrdzojx/flash-attn_6c209a798d30416590e3a675176214d5/csrc/flash_attn/src/flash_bwd_hdim128_fp16_sm80.cu -o /tmp/pip-install-xwrdzojx/flash-attn_6c209a798d30416590e3a675176214d5/build/temp.linux-x86_64-3.10/csrc/flash_attn/src/flash_bwd_hdim128_fp16_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math -gencode arch=compute_80,code=sm_80 -gencode arch=compute_90,code=sm_90 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=flash_attn_2_cuda -D_GLIBCXX_USE_CXX11_ABI=0\n",
      "  [4/49] /usr/local/cuda-12.1/bin/nvcc --generate-dependencies-with-compile --dependency-output /tmp/pip-install-xwrdzojx/flash-attn_6c209a798d30416590e3a675176214d5/build/temp.linux-x86_64-3.10/csrc/flash_attn/src/flash_bwd_hdim160_bf16_sm80.o.d -I/tmp/pip-install-xwrdzojx/flash-attn_6c209a798d30416590e3a675176214d5/csrc/flash_attn -I/tmp/pip-install-xwrdzojx/flash-attn_6c209a798d30416590e3a675176214d5/csrc/flash_attn/src -I/tmp/pip-install-xwrdzojx/flash-attn_6c209a798d30416590e3a675176214d5/csrc/cutlass/include -I/home/ubuntu/.local/lib/python3.10/site-packages/torch/include -I/home/ubuntu/.local/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/home/ubuntu/.local/lib/python3.10/site-packages/torch/include/TH -I/home/ubuntu/.local/lib/python3.10/site-packages/torch/include/THC -I/usr/local/cuda-12.1/include -I/usr/include/python3.10 -c -c /tmp/pip-install-xwrdzojx/flash-attn_6c209a798d30416590e3a675176214d5/csrc/flash_attn/src/flash_bwd_hdim160_bf16_sm80.cu -o /tmp/pip-install-xwrdzojx/flash-attn_6c209a798d30416590e3a675176214d5/build/temp.linux-x86_64-3.10/csrc/flash_attn/src/flash_bwd_hdim160_bf16_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math -gencode arch=compute_80,code=sm_80 -gencode arch=compute_90,code=sm_90 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=flash_attn_2_cuda -D_GLIBCXX_USE_CXX11_ABI=0\n",
      "  [5/49] /usr/local/cuda-12.1/bin/nvcc --generate-dependencies-with-compile --dependency-output /tmp/pip-install-xwrdzojx/flash-attn_6c209a798d30416590e3a675176214d5/build/temp.linux-x86_64-3.10/csrc/flash_attn/src/flash_bwd_hdim160_fp16_sm80.o.d -I/tmp/pip-install-xwrdzojx/flash-attn_6c209a798d30416590e3a675176214d5/csrc/flash_attn -I/tmp/pip-install-xwrdzojx/flash-attn_6c209a798d30416590e3a675176214d5/csrc/flash_attn/src -I/tmp/pip-install-xwrdzojx/flash-attn_6c209a798d30416590e3a675176214d5/csrc/cutlass/include -I/home/ubuntu/.local/lib/python3.10/site-packages/torch/include -I/home/ubuntu/.local/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/home/ubuntu/.local/lib/python3.10/site-packages/torch/include/TH -I/home/ubuntu/.local/lib/python3.10/site-packages/torch/include/THC -I/usr/local/cuda-12.1/include -I/usr/include/python3.10 -c -c /tmp/pip-install-xwrdzojx/flash-attn_6c209a798d30416590e3a675176214d5/csrc/flash_attn/src/flash_bwd_hdim160_fp16_sm80.cu -o /tmp/pip-install-xwrdzojx/flash-attn_6c209a798d30416590e3a675176214d5/build/temp.linux-x86_64-3.10/csrc/flash_attn/src/flash_bwd_hdim160_fp16_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math -gencode arch=compute_80,code=sm_80 -gencode arch=compute_90,code=sm_90 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=flash_attn_2_cuda -D_GLIBCXX_USE_CXX11_ABI=0\n",
      "  [6/49] /usr/local/cuda-12.1/bin/nvcc --generate-dependencies-with-compile --dependency-output /tmp/pip-install-xwrdzojx/flash-attn_6c209a798d30416590e3a675176214d5/build/temp.linux-x86_64-3.10/csrc/flash_attn/src/flash_bwd_hdim192_bf16_sm80.o.d -I/tmp/pip-install-xwrdzojx/flash-attn_6c209a798d30416590e3a675176214d5/csrc/flash_attn -I/tmp/pip-install-xwrdzojx/flash-attn_6c209a798d30416590e3a675176214d5/csrc/flash_attn/src -I/tmp/pip-install-xwrdzojx/flash-attn_6c209a798d30416590e3a675176214d5/csrc/cutlass/include -I/home/ubuntu/.local/lib/python3.10/site-packages/torch/include -I/home/ubuntu/.local/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/home/ubuntu/.local/lib/python3.10/site-packages/torch/include/TH -I/home/ubuntu/.local/lib/python3.10/site-packages/torch/include/THC -I/usr/local/cuda-12.1/include -I/usr/include/python3.10 -c -c /tmp/pip-install-xwrdzojx/flash-attn_6c209a798d30416590e3a675176214d5/csrc/flash_attn/src/flash_bwd_hdim192_bf16_sm80.cu -o /tmp/pip-install-xwrdzojx/flash-attn_6c209a798d30416590e3a675176214d5/build/temp.linux-x86_64-3.10/csrc/flash_attn/src/flash_bwd_hdim192_bf16_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math -gencode arch=compute_80,code=sm_80 -gencode arch=compute_90,code=sm_90 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=flash_attn_2_cuda -D_GLIBCXX_USE_CXX11_ABI=0\n",
      "  [7/49] /usr/local/cuda-12.1/bin/nvcc --generate-dependencies-with-compile --dependency-output /tmp/pip-install-xwrdzojx/flash-attn_6c209a798d30416590e3a675176214d5/build/temp.linux-x86_64-3.10/csrc/flash_attn/src/flash_bwd_hdim192_fp16_sm80.o.d -I/tmp/pip-install-xwrdzojx/flash-attn_6c209a798d30416590e3a675176214d5/csrc/flash_attn -I/tmp/pip-install-xwrdzojx/flash-attn_6c209a798d30416590e3a675176214d5/csrc/flash_attn/src -I/tmp/pip-install-xwrdzojx/flash-attn_6c209a798d30416590e3a675176214d5/csrc/cutlass/include -I/home/ubuntu/.local/lib/python3.10/site-packages/torch/include -I/home/ubuntu/.local/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/home/ubuntu/.local/lib/python3.10/site-packages/torch/include/TH -I/home/ubuntu/.local/lib/python3.10/site-packages/torch/include/THC -I/usr/local/cuda-12.1/include -I/usr/include/python3.10 -c -c /tmp/pip-install-xwrdzojx/flash-attn_6c209a798d30416590e3a675176214d5/csrc/flash_attn/src/flash_bwd_hdim192_fp16_sm80.cu -o /tmp/pip-install-xwrdzojx/flash-attn_6c209a798d30416590e3a675176214d5/build/temp.linux-x86_64-3.10/csrc/flash_attn/src/flash_bwd_hdim192_fp16_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math -gencode arch=compute_80,code=sm_80 -gencode arch=compute_90,code=sm_90 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=flash_attn_2_cuda -D_GLIBCXX_USE_CXX11_ABI=0\n",
      "  [8/49] /usr/local/cuda-12.1/bin/nvcc --generate-dependencies-with-compile --dependency-output /tmp/pip-install-xwrdzojx/flash-attn_6c209a798d30416590e3a675176214d5/build/temp.linux-x86_64-3.10/csrc/flash_attn/src/flash_bwd_hdim224_bf16_sm80.o.d -I/tmp/pip-install-xwrdzojx/flash-attn_6c209a798d30416590e3a675176214d5/csrc/flash_attn -I/tmp/pip-install-xwrdzojx/flash-attn_6c209a798d30416590e3a675176214d5/csrc/flash_attn/src -I/tmp/pip-install-xwrdzojx/flash-attn_6c209a798d30416590e3a675176214d5/csrc/cutlass/include -I/home/ubuntu/.local/lib/python3.10/site-packages/torch/include -I/home/ubuntu/.local/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/home/ubuntu/.local/lib/python3.10/site-packages/torch/include/TH -I/home/ubuntu/.local/lib/python3.10/site-packages/torch/include/THC -I/usr/local/cuda-12.1/include -I/usr/include/python3.10 -c -c /tmp/pip-install-xwrdzojx/flash-attn_6c209a798d30416590e3a675176214d5/csrc/flash_attn/src/flash_bwd_hdim224_bf16_sm80.cu -o /tmp/pip-install-xwrdzojx/flash-attn_6c209a798d30416590e3a675176214d5/build/temp.linux-x86_64-3.10/csrc/flash_attn/src/flash_bwd_hdim224_bf16_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math -gencode arch=compute_80,code=sm_80 -gencode arch=compute_90,code=sm_90 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=flash_attn_2_cuda -D_GLIBCXX_USE_CXX11_ABI=0\n",
      "  [9/49] /usr/local/cuda-12.1/bin/nvcc --generate-dependencies-with-compile --dependency-output /tmp/pip-install-xwrdzojx/flash-attn_6c209a798d30416590e3a675176214d5/build/temp.linux-x86_64-3.10/csrc/flash_attn/src/flash_bwd_hdim224_fp16_sm80.o.d -I/tmp/pip-install-xwrdzojx/flash-attn_6c209a798d30416590e3a675176214d5/csrc/flash_attn -I/tmp/pip-install-xwrdzojx/flash-attn_6c209a798d30416590e3a675176214d5/csrc/flash_attn/src -I/tmp/pip-install-xwrdzojx/flash-attn_6c209a798d30416590e3a675176214d5/csrc/cutlass/include -I/home/ubuntu/.local/lib/python3.10/site-packages/torch/include -I/home/ubuntu/.local/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/home/ubuntu/.local/lib/python3.10/site-packages/torch/include/TH -I/home/ubuntu/.local/lib/python3.10/site-packages/torch/include/THC -I/usr/local/cuda-12.1/include -I/usr/include/python3.10 -c -c /tmp/pip-install-xwrdzojx/flash-attn_6c209a798d30416590e3a675176214d5/csrc/flash_attn/src/flash_bwd_hdim224_fp16_sm80.cu -o /tmp/pip-install-xwrdzojx/flash-attn_6c209a798d30416590e3a675176214d5/build/temp.linux-x86_64-3.10/csrc/flash_attn/src/flash_bwd_hdim224_fp16_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math -gencode arch=compute_80,code=sm_80 -gencode arch=compute_90,code=sm_90 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=flash_attn_2_cuda -D_GLIBCXX_USE_CXX11_ABI=0\n",
      "  [10/49] /usr/local/cuda-12.1/bin/nvcc --generate-dependencies-with-compile --dependency-output /tmp/pip-install-xwrdzojx/flash-attn_6c209a798d30416590e3a675176214d5/build/temp.linux-x86_64-3.10/csrc/flash_attn/src/flash_bwd_hdim256_bf16_sm80.o.d -I/tmp/pip-install-xwrdzojx/flash-attn_6c209a798d30416590e3a675176214d5/csrc/flash_attn -I/tmp/pip-install-xwrdzojx/flash-attn_6c209a798d30416590e3a675176214d5/csrc/flash_attn/src -I/tmp/pip-install-xwrdzojx/flash-attn_6c209a798d30416590e3a675176214d5/csrc/cutlass/include -I/home/ubuntu/.local/lib/python3.10/site-packages/torch/include -I/home/ubuntu/.local/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/home/ubuntu/.local/lib/python3.10/site-packages/torch/include/TH -I/home/ubuntu/.local/lib/python3.10/site-packages/torch/include/THC -I/usr/local/cuda-12.1/include -I/usr/include/python3.10 -c -c /tmp/pip-install-xwrdzojx/flash-attn_6c209a798d30416590e3a675176214d5/csrc/flash_attn/src/flash_bwd_hdim256_bf16_sm80.cu -o /tmp/pip-install-xwrdzojx/flash-attn_6c209a798d30416590e3a675176214d5/build/temp.linux-x86_64-3.10/csrc/flash_attn/src/flash_bwd_hdim256_bf16_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math -gencode arch=compute_80,code=sm_80 -gencode arch=compute_90,code=sm_90 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=flash_attn_2_cuda -D_GLIBCXX_USE_CXX11_ABI=0\n",
      "  [11/49] /usr/local/cuda-12.1/bin/nvcc --generate-dependencies-with-compile --dependency-output /tmp/pip-install-xwrdzojx/flash-attn_6c209a798d30416590e3a675176214d5/build/temp.linux-x86_64-3.10/csrc/flash_attn/src/flash_bwd_hdim256_fp16_sm80.o.d -I/tmp/pip-install-xwrdzojx/flash-attn_6c209a798d30416590e3a675176214d5/csrc/flash_attn -I/tmp/pip-install-xwrdzojx/flash-attn_6c209a798d30416590e3a675176214d5/csrc/flash_attn/src -I/tmp/pip-install-xwrdzojx/flash-attn_6c209a798d30416590e3a675176214d5/csrc/cutlass/include -I/home/ubuntu/.local/lib/python3.10/site-packages/torch/include -I/home/ubuntu/.local/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/home/ubuntu/.local/lib/python3.10/site-packages/torch/include/TH -I/home/ubuntu/.local/lib/python3.10/site-packages/torch/include/THC -I/usr/local/cuda-12.1/include -I/usr/include/python3.10 -c -c /tmp/pip-install-xwrdzojx/flash-attn_6c209a798d30416590e3a675176214d5/csrc/flash_attn/src/flash_bwd_hdim256_fp16_sm80.cu -o /tmp/pip-install-xwrdzojx/flash-attn_6c209a798d30416590e3a675176214d5/build/temp.linux-x86_64-3.10/csrc/flash_attn/src/flash_bwd_hdim256_fp16_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math -gencode arch=compute_80,code=sm_80 -gencode arch=compute_90,code=sm_90 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=flash_attn_2_cuda -D_GLIBCXX_USE_CXX11_ABI=0\n",
      "  [12/49] /usr/local/cuda-12.1/bin/nvcc --generate-dependencies-with-compile --dependency-output /tmp/pip-install-xwrdzojx/flash-attn_6c209a798d30416590e3a675176214d5/build/temp.linux-x86_64-3.10/csrc/flash_attn/src/flash_bwd_hdim32_bf16_sm80.o.d -I/tmp/pip-install-xwrdzojx/flash-attn_6c209a798d30416590e3a675176214d5/csrc/flash_attn -I/tmp/pip-install-xwrdzojx/flash-attn_6c209a798d30416590e3a675176214d5/csrc/flash_attn/src -I/tmp/pip-install-xwrdzojx/flash-attn_6c209a798d30416590e3a675176214d5/csrc/cutlass/include -I/home/ubuntu/.local/lib/python3.10/site-packages/torch/include -I/home/ubuntu/.local/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/home/ubuntu/.local/lib/python3.10/site-packages/torch/include/TH -I/home/ubuntu/.local/lib/python3.10/site-packages/torch/include/THC -I/usr/local/cuda-12.1/include -I/usr/include/python3.10 -c -c /tmp/pip-install-xwrdzojx/flash-attn_6c209a798d30416590e3a675176214d5/csrc/flash_attn/src/flash_bwd_hdim32_bf16_sm80.cu -o /tmp/pip-install-xwrdzojx/flash-attn_6c209a798d30416590e3a675176214d5/build/temp.linux-x86_64-3.10/csrc/flash_attn/src/flash_bwd_hdim32_bf16_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math -gencode arch=compute_80,code=sm_80 -gencode arch=compute_90,code=sm_90 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=flash_attn_2_cuda -D_GLIBCXX_USE_CXX11_ABI=0\n",
      "  [13/49] /usr/local/cuda-12.1/bin/nvcc --generate-dependencies-with-compile --dependency-output /tmp/pip-install-xwrdzojx/flash-attn_6c209a798d30416590e3a675176214d5/build/temp.linux-x86_64-3.10/csrc/flash_attn/src/flash_bwd_hdim32_fp16_sm80.o.d -I/tmp/pip-install-xwrdzojx/flash-attn_6c209a798d30416590e3a675176214d5/csrc/flash_attn -I/tmp/pip-install-xwrdzojx/flash-attn_6c209a798d30416590e3a675176214d5/csrc/flash_attn/src -I/tmp/pip-install-xwrdzojx/flash-attn_6c209a798d30416590e3a675176214d5/csrc/cutlass/include -I/home/ubuntu/.local/lib/python3.10/site-packages/torch/include -I/home/ubuntu/.local/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/home/ubuntu/.local/lib/python3.10/site-packages/torch/include/TH -I/home/ubuntu/.local/lib/python3.10/site-packages/torch/include/THC -I/usr/local/cuda-12.1/include -I/usr/include/python3.10 -c -c /tmp/pip-install-xwrdzojx/flash-attn_6c209a798d30416590e3a675176214d5/csrc/flash_attn/src/flash_bwd_hdim32_fp16_sm80.cu -o /tmp/pip-install-xwrdzojx/flash-attn_6c209a798d30416590e3a675176214d5/build/temp.linux-x86_64-3.10/csrc/flash_attn/src/flash_bwd_hdim32_fp16_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math -gencode arch=compute_80,code=sm_80 -gencode arch=compute_90,code=sm_90 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=flash_attn_2_cuda -D_GLIBCXX_USE_CXX11_ABI=0\n",
      "  [14/49] /usr/local/cuda-12.1/bin/nvcc --generate-dependencies-with-compile --dependency-output /tmp/pip-install-xwrdzojx/flash-attn_6c209a798d30416590e3a675176214d5/build/temp.linux-x86_64-3.10/csrc/flash_attn/src/flash_bwd_hdim64_bf16_sm80.o.d -I/tmp/pip-install-xwrdzojx/flash-attn_6c209a798d30416590e3a675176214d5/csrc/flash_attn -I/tmp/pip-install-xwrdzojx/flash-attn_6c209a798d30416590e3a675176214d5/csrc/flash_attn/src -I/tmp/pip-install-xwrdzojx/flash-attn_6c209a798d30416590e3a675176214d5/csrc/cutlass/include -I/home/ubuntu/.local/lib/python3.10/site-packages/torch/include -I/home/ubuntu/.local/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/home/ubuntu/.local/lib/python3.10/site-packages/torch/include/TH -I/home/ubuntu/.local/lib/python3.10/site-packages/torch/include/THC -I/usr/local/cuda-12.1/include -I/usr/include/python3.10 -c -c /tmp/pip-install-xwrdzojx/flash-attn_6c209a798d30416590e3a675176214d5/csrc/flash_attn/src/flash_bwd_hdim64_bf16_sm80.cu -o /tmp/pip-install-xwrdzojx/flash-attn_6c209a798d30416590e3a675176214d5/build/temp.linux-x86_64-3.10/csrc/flash_attn/src/flash_bwd_hdim64_bf16_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math -gencode arch=compute_80,code=sm_80 -gencode arch=compute_90,code=sm_90 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=flash_attn_2_cuda -D_GLIBCXX_USE_CXX11_ABI=0\n",
      "  [15/49] /usr/local/cuda-12.1/bin/nvcc --generate-dependencies-with-compile --dependency-output /tmp/pip-install-xwrdzojx/flash-attn_6c209a798d30416590e3a675176214d5/build/temp.linux-x86_64-3.10/csrc/flash_attn/src/flash_bwd_hdim64_fp16_sm80.o.d -I/tmp/pip-install-xwrdzojx/flash-attn_6c209a798d30416590e3a675176214d5/csrc/flash_attn -I/tmp/pip-install-xwrdzojx/flash-attn_6c209a798d30416590e3a675176214d5/csrc/flash_attn/src -I/tmp/pip-install-xwrdzojx/flash-attn_6c209a798d30416590e3a675176214d5/csrc/cutlass/include -I/home/ubuntu/.local/lib/python3.10/site-packages/torch/include -I/home/ubuntu/.local/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/home/ubuntu/.local/lib/python3.10/site-packages/torch/include/TH -I/home/ubuntu/.local/lib/python3.10/site-packages/torch/include/THC -I/usr/local/cuda-12.1/include -I/usr/include/python3.10 -c -c /tmp/pip-install-xwrdzojx/flash-attn_6c209a798d30416590e3a675176214d5/csrc/flash_attn/src/flash_bwd_hdim64_fp16_sm80.cu -o /tmp/pip-install-xwrdzojx/flash-attn_6c209a798d30416590e3a675176214d5/build/temp.linux-x86_64-3.10/csrc/flash_attn/src/flash_bwd_hdim64_fp16_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math -gencode arch=compute_80,code=sm_80 -gencode arch=compute_90,code=sm_90 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=flash_attn_2_cuda -D_GLIBCXX_USE_CXX11_ABI=0\n",
      "  [16/49] /usr/local/cuda-12.1/bin/nvcc --generate-dependencies-with-compile --dependency-output /tmp/pip-install-xwrdzojx/flash-attn_6c209a798d30416590e3a675176214d5/build/temp.linux-x86_64-3.10/csrc/flash_attn/src/flash_bwd_hdim96_bf16_sm80.o.d -I/tmp/pip-install-xwrdzojx/flash-attn_6c209a798d30416590e3a675176214d5/csrc/flash_attn -I/tmp/pip-install-xwrdzojx/flash-attn_6c209a798d30416590e3a675176214d5/csrc/flash_attn/src -I/tmp/pip-install-xwrdzojx/flash-attn_6c209a798d30416590e3a675176214d5/csrc/cutlass/include -I/home/ubuntu/.local/lib/python3.10/site-packages/torch/include -I/home/ubuntu/.local/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/home/ubuntu/.local/lib/python3.10/site-packages/torch/include/TH -I/home/ubuntu/.local/lib/python3.10/site-packages/torch/include/THC -I/usr/local/cuda-12.1/include -I/usr/include/python3.10 -c -c /tmp/pip-install-xwrdzojx/flash-attn_6c209a798d30416590e3a675176214d5/csrc/flash_attn/src/flash_bwd_hdim96_bf16_sm80.cu -o /tmp/pip-install-xwrdzojx/flash-attn_6c209a798d30416590e3a675176214d5/build/temp.linux-x86_64-3.10/csrc/flash_attn/src/flash_bwd_hdim96_bf16_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math -gencode arch=compute_80,code=sm_80 -gencode arch=compute_90,code=sm_90 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=flash_attn_2_cuda -D_GLIBCXX_USE_CXX11_ABI=0\n",
      "  [17/49] /usr/local/cuda-12.1/bin/nvcc --generate-dependencies-with-compile --dependency-output /tmp/pip-install-xwrdzojx/flash-attn_6c209a798d30416590e3a675176214d5/build/temp.linux-x86_64-3.10/csrc/flash_attn/src/flash_bwd_hdim96_fp16_sm80.o.d -I/tmp/pip-install-xwrdzojx/flash-attn_6c209a798d30416590e3a675176214d5/csrc/flash_attn -I/tmp/pip-install-xwrdzojx/flash-attn_6c209a798d30416590e3a675176214d5/csrc/flash_attn/src -I/tmp/pip-install-xwrdzojx/flash-attn_6c209a798d30416590e3a675176214d5/csrc/cutlass/include -I/home/ubuntu/.local/lib/python3.10/site-packages/torch/include -I/home/ubuntu/.local/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/home/ubuntu/.local/lib/python3.10/site-packages/torch/include/TH -I/home/ubuntu/.local/lib/python3.10/site-packages/torch/include/THC -I/usr/local/cuda-12.1/include -I/usr/include/python3.10 -c -c /tmp/pip-install-xwrdzojx/flash-attn_6c209a798d30416590e3a675176214d5/csrc/flash_attn/src/flash_bwd_hdim96_fp16_sm80.cu -o /tmp/pip-install-xwrdzojx/flash-attn_6c209a798d30416590e3a675176214d5/build/temp.linux-x86_64-3.10/csrc/flash_attn/src/flash_bwd_hdim96_fp16_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math -gencode arch=compute_80,code=sm_80 -gencode arch=compute_90,code=sm_90 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=flash_attn_2_cuda -D_GLIBCXX_USE_CXX11_ABI=0\n",
      "  [18/49] /usr/local/cuda-12.1/bin/nvcc --generate-dependencies-with-compile --dependency-output /tmp/pip-install-xwrdzojx/flash-attn_6c209a798d30416590e3a675176214d5/build/temp.linux-x86_64-3.10/csrc/flash_attn/src/flash_fwd_hdim128_bf16_sm80.o.d -I/tmp/pip-install-xwrdzojx/flash-attn_6c209a798d30416590e3a675176214d5/csrc/flash_attn -I/tmp/pip-install-xwrdzojx/flash-attn_6c209a798d30416590e3a675176214d5/csrc/flash_attn/src -I/tmp/pip-install-xwrdzojx/flash-attn_6c209a798d30416590e3a675176214d5/csrc/cutlass/include -I/home/ubuntu/.local/lib/python3.10/site-packages/torch/include -I/home/ubuntu/.local/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/home/ubuntu/.local/lib/python3.10/site-packages/torch/include/TH -I/home/ubuntu/.local/lib/python3.10/site-packages/torch/include/THC -I/usr/local/cuda-12.1/include -I/usr/include/python3.10 -c -c /tmp/pip-install-xwrdzojx/flash-attn_6c209a798d30416590e3a675176214d5/csrc/flash_attn/src/flash_fwd_hdim128_bf16_sm80.cu -o /tmp/pip-install-xwrdzojx/flash-attn_6c209a798d30416590e3a675176214d5/build/temp.linux-x86_64-3.10/csrc/flash_attn/src/flash_fwd_hdim128_bf16_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math -gencode arch=compute_80,code=sm_80 -gencode arch=compute_90,code=sm_90 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=flash_attn_2_cuda -D_GLIBCXX_USE_CXX11_ABI=0\n",
      "  [19/49] /usr/local/cuda-12.1/bin/nvcc --generate-dependencies-with-compile --dependency-output /tmp/pip-install-xwrdzojx/flash-attn_6c209a798d30416590e3a675176214d5/build/temp.linux-x86_64-3.10/csrc/flash_attn/src/flash_fwd_hdim128_fp16_sm80.o.d -I/tmp/pip-install-xwrdzojx/flash-attn_6c209a798d30416590e3a675176214d5/csrc/flash_attn -I/tmp/pip-install-xwrdzojx/flash-attn_6c209a798d30416590e3a675176214d5/csrc/flash_attn/src -I/tmp/pip-install-xwrdzojx/flash-attn_6c209a798d30416590e3a675176214d5/csrc/cutlass/include -I/home/ubuntu/.local/lib/python3.10/site-packages/torch/include -I/home/ubuntu/.local/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/home/ubuntu/.local/lib/python3.10/site-packages/torch/include/TH -I/home/ubuntu/.local/lib/python3.10/site-packages/torch/include/THC -I/usr/local/cuda-12.1/include -I/usr/include/python3.10 -c -c /tmp/pip-install-xwrdzojx/flash-attn_6c209a798d30416590e3a675176214d5/csrc/flash_attn/src/flash_fwd_hdim128_fp16_sm80.cu -o /tmp/pip-install-xwrdzojx/flash-attn_6c209a798d30416590e3a675176214d5/build/temp.linux-x86_64-3.10/csrc/flash_attn/src/flash_fwd_hdim128_fp16_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math -gencode arch=compute_80,code=sm_80 -gencode arch=compute_90,code=sm_90 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=flash_attn_2_cuda -D_GLIBCXX_USE_CXX11_ABI=0\n"
     ]
    }
   ],
   "source": [
    "# 1. First, uninstall flash-attention\n",
    "!pip uninstall flash-attn -y\n",
    "\n",
    "# 2. Clear the PyTorch extensions cache\n",
    "!rm -rf ~/.cache/torch_extensions\n",
    "\n",
    "# 3. Install build dependencies\n",
    "!pip install ninja packaging\n",
    "\n",
    "# 4. Reinstall flash-attention with verbose output to see any issues\n",
    "!MAX_JOBS=3 pip install flash-attn==2.4.2 --no-build-isolation --verbose\n",
    "# !MAX_JOBS=4 pip install flash-attn --no-build-isolation --verbose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "!git config --global credential.helper store\n",
    "\n",
    "login(\n",
    "  token=\"\", # remember to add token\n",
    "  add_to_git_credential=True\n",
    ")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting numpy==1.24.3\n",
      "  Using cached numpy-1.24.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
      "Using cached numpy-1.24.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.24.3\n",
      "    Uninstalling numpy-1.24.3:\n",
      "      Successfully uninstalled numpy-1.24.3\n",
      "Successfully installed numpy-1.24.3\n",
      "1.24.3\n"
     ]
    }
   ],
   "source": [
    "# Try skipping since we upgraded many of the other libraries\n",
    "#\n",
    "# Found that 1.24.3 was needed to avoid issues with datasets\n",
    "# !pip install numpy==1.24.3 --force-reinstall --no-deps\n",
    "\n",
    "# Verify the version\n",
    "#import numpy as np\n",
    "#print(np.version.version)  # Should now show 1.24.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "caa9c204eb984c7b866cec66a687f995",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'content': 'You are an text to SQL query translator. Users will ask you questions in English and you will generate a SQL query based on the provided SCHEMA.\\nSCHEMA:\\nCREATE TABLE table_name_61 (record VARCHAR, game VARCHAR)', 'role': 'system'}, {'content': 'What was the record for game 59?', 'role': 'user'}, {'content': 'SELECT record FROM table_name_61 WHERE game = 59', 'role': 'assistant'}]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1ae78feb372464c8f23ac401b6edabb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/10 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2c01f3a532a4121bf24b00fcc831e44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1191706"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    " \n",
    "# Convert dataset to OAI messages\n",
    "system_message = \"\"\"You are an text to SQL query translator. Users will ask you questions in English and you will generate a SQL query based on the provided SCHEMA.\n",
    "SCHEMA:\n",
    "{schema}\"\"\"\n",
    " \n",
    "def create_conversation(sample):\n",
    "  return {\n",
    "    \"messages\": [\n",
    "      {\"role\": \"system\", \"content\": system_message.format(schema=sample[\"context\"])},\n",
    "      {\"role\": \"user\", \"content\": sample[\"question\"]},\n",
    "      {\"role\": \"assistant\", \"content\": sample[\"answer\"]}\n",
    "    ]\n",
    "  }\n",
    " \n",
    "# Load dataset from the hub\n",
    "dataset = load_dataset(\"b-mc2/sql-create-context\", split=\"train\")\n",
    "dataset = dataset.shuffle().select(range(12500))\n",
    " \n",
    "# Convert dataset to OAI messages\n",
    "dataset = dataset.map(create_conversation, remove_columns=dataset.features,batched=False)\n",
    "# split dataset into 10,000 training samples and 2,500 test samples\n",
    "dataset = dataset.train_test_split(test_size=2500/12500)\n",
    " \n",
    "print(dataset[\"train\"][345][\"messages\"])\n",
    " \n",
    "# save datasets to disk\n",
    "dataset[\"train\"].to_json(\"train_dataset.json\", orient=\"records\")\n",
    "dataset[\"test\"].to_json(\"test_dataset.json\", orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57c0607662044afc9a48113295b2689f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    " \n",
    "# Load jsonl data from disk\n",
    "dataset = load_dataset(\"json\", data_files=\"train_dataset.json\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.4.0+cu121\n",
      "CUDA version: 12.1\n",
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2023 NVIDIA Corporation\n",
      "Built on Mon_Apr__3_17:16:06_PDT_2023\n",
      "Cuda compilation tools, release 12.1, V12.1.105\n",
      "Build cuda_12.1.r12.1/compiler.32688072_0\n",
      "Tue Feb 11 19:34:43 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 570.86.15              Driver Version: 570.86.15      CUDA Version: 12.8     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A10G                    Off |   00000000:00:1E.0 Off |                    0 |\n",
      "|  0%   25C    P8             15W /  300W |       1MiB /  23028MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    "!nvcc --version\n",
    "\n",
    "# Set the environment variable for memory management - training run suggested Python fragmentation causing out of GPU memory errors\n",
    "import os\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "# delete any existing model and tokenizer objects\n",
    "import gc\n",
    "try:\n",
    "    del model\n",
    "    del tokenizer\n",
    "except:\n",
    "    pass\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "\n",
    "# Clear CUDA cache\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Verify memory was freed\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4868400bdf094349b8903bd17a48b932",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from trl import setup_chat_format\n",
    " \n",
    "# Hugging Face model id\n",
    "model_id = \"meta-llama/Meta-Llama-3.1-8B\" # or \"mistralai/Mistral-7B-v0.1\" \n",
    " \n",
    "# BitsAndBytesConfig int-4 config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    " \n",
    "# Load model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    quantization_config=bnb_config,\n",
    "    max_memory={0: \"21GB\"}  # Reserve some memory for gradients\n",
    ")\n",
    "model.gradient_checkpointing_enable()\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.padding_side = 'right' # to prevent warnings\n",
    " \n",
    "# # set chat template to OAI chatML, remove if you start from a fine-tuned model\n",
    "model, tokenizer = setup_chat_format(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig\n",
    " \n",
    "# LoRA config based on QLoRA paper & Sebastian Raschka experiment\n",
    "peft_config = LoraConfig(\n",
    "        lora_alpha=128,\n",
    "        lora_dropout=0.05,\n",
    "        r=256,\n",
    "        bias=\"none\",\n",
    "        target_modules=\"all-linear\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "\n",
    "from transformers import TrainingArguments\n",
    " \n",
    "args = TrainingArguments(\n",
    "    output_dir=model_id+\"-text-to-sql\",     # directory to save and repository id\n",
    "    num_train_epochs=3,                     # number of training epochs\n",
    "    per_device_train_batch_size=3,          # batch size per device during training\n",
    "    gradient_accumulation_steps=2,          # number of steps before performing a backward/update pass\n",
    "    gradient_checkpointing=True,            # use gradient checkpointing to save memory\n",
    "    optim=\"adamw_torch_fused\",              # use fused adamw optimizer\n",
    "    logging_steps=10,                       # log every 10 steps\n",
    "    save_strategy=\"epoch\",                  # save checkpoint every epoch\n",
    "    learning_rate=2e-4,                     # learning rate, based on QLoRA paper\n",
    "    bf16=True,                              # use bfloat16 precision\n",
    "    tf32=True,                              # use tf32 precision\n",
    "    max_grad_norm=0.3,                      # max gradient norm based on QLoRA paper\n",
    "    warmup_ratio=0.03,                      # warmup ratio based on QLoRA paper\n",
    "    lr_scheduler_type=\"constant\",           # use constant learning rate scheduler\n",
    "    push_to_hub=True,                       # push model to hub\n",
    "    report_to=\"tensorboard\",                # report metrics to tensorboard\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch: 2.4.0+cu121\n",
      "Transformers: 4.44.2\n",
      "TRL: 0.9.6\n"
     ]
    }
   ],
   "source": [
    "# Check current versions\n",
    "import torch\n",
    "import transformers\n",
    "import trl\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"Transformers: {transformers.__version__}\")\n",
    "print(f\"TRL: {trl.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length, packing, dataset_kwargs. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:192: UserWarning: You passed a `packing` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:280: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:366: UserWarning: You passed a `dataset_kwargs` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91ba42c35b0544a1ab92bb40618ba2ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# If you get an error about packing here, you need to upgrade jinja2, which is a dependency of trl\n",
    "# !pip install \"jinja2>=3.1.0\" --upgrade\n",
    "\n",
    "from trl import SFTTrainer\n",
    " \n",
    "max_seq_length = 3072 # max sequence length for model and packing of the dataset\n",
    " \n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    "    max_seq_length=max_seq_length,\n",
    "    tokenizer=tokenizer,\n",
    "    packing=True,\n",
    "    dataset_kwargs={\n",
    "        \"add_special_tokens\": False,  # We template with special tokens\n",
    "        \"append_concat_token\": False, # No need to add additional separator token\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "{'messages': [{'content': 'You are an text to SQL query translator. Users will ask you questions in English and you will generate a SQL query based on the provided SCHEMA.\\nSCHEMA:\\nCREATE TABLE table_1057262_2 (tasmania VARCHAR, new_south_wales VARCHAR)', 'role': 'system'}, {'content': \"what's the total number of\\xa0tasmania\\xa0with\\xa0new south wales\\xa0crop of 190 kilotonnes\", 'role': 'user'}, {'content': 'SELECT COUNT(tasmania) FROM table_1057262_2 WHERE new_south_wales = 190', 'role': 'assistant'}]}\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset))\n",
    "print(dataset[1])  # Look at first example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "The input hidden states seems to be silently casted in float32, this might be related to the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in torch.bfloat16.\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 4.40 GiB. GPU 0 has a total capacity of 22.07 GiB of which 3.42 GiB is free. Including non-PyTorch memory, this process has 18.64 GiB memory in use. Of the allocated memory 18.19 GiB is allocated by PyTorch, and 164.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# start training, the model will be automatically saved to the hub and the output directory\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# save model\u001b[39;00m\n\u001b[1;32m      5\u001b[0m trainer\u001b[38;5;241m.\u001b[39msave_model()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:451\u001b[0m, in \u001b[0;36mSFTTrainer.train\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneftune_noise_alpha \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer_supports_neftune:\n\u001b[1;32m    449\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trl_activate_neftune(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[0;32m--> 451\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;66;03m# After training we make sure to retrieve back the original forward pass method\u001b[39;00m\n\u001b[1;32m    454\u001b[0m \u001b[38;5;66;03m# for the embedding layer by removing the forward post hook.\u001b[39;00m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneftune_noise_alpha \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer_supports_neftune:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:1929\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1926\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1927\u001b[0m     \u001b[38;5;66;03m# Disable progress bars when uploading models during checkpoints to avoid polluting stdout\u001b[39;00m\n\u001b[1;32m   1928\u001b[0m     hf_hub_utils\u001b[38;5;241m.\u001b[39mdisable_progress_bars()\n\u001b[0;32m-> 1929\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1930\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1931\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1932\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1933\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1934\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1935\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1936\u001b[0m     hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:2279\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2276\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   2278\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 2279\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2281\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2282\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2283\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2284\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2285\u001b[0m ):\n\u001b[1;32m   2286\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2287\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:3318\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   3315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   3317\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 3318\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3320\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[1;32m   3321\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   3322\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3323\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   3324\u001b[0m ):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:3363\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   3361\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3362\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 3363\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3364\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   3365\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   3366\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/accelerate/utils/operations.py:819\u001b[0m, in \u001b[0;36mconvert_outputs_to_fp32.<locals>.forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 819\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/accelerate/utils/operations.py:807\u001b[0m, in \u001b[0;36mConvertOutputsToFp32.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    806\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 807\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/amp/autocast_mode.py:43\u001b[0m, in \u001b[0;36mautocast_decorator.<locals>.decorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_autocast\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[0;32m---> 43\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/peft/peft_model.py:1577\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[0m\n\u001b[1;32m   1575\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_peft_forward_hooks(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1576\u001b[0m         kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_peft_forward_args}\n\u001b[0;32m-> 1577\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1578\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1579\u001b[0m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1580\u001b[0m \u001b[43m            \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1581\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1582\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1583\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1584\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1585\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1586\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1588\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m _get_batch_size(input_ids, inputs_embeds)\n\u001b[1;32m   1589\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1590\u001b[0m     \u001b[38;5;66;03m# concat prompt attention mask\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/peft/tuners/tuners_utils.py:188\u001b[0m, in \u001b[0;36mBaseTuner.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any):\n\u001b[0;32m--> 188\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/accelerate/hooks.py:169\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:1214\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1211\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1212\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1213\u001b[0m     \u001b[38;5;66;03m# Shift so that tokens < n predict n\u001b[39;00m\n\u001b[0;32m-> 1214\u001b[0m     shift_logits \u001b[38;5;241m=\u001b[39m \u001b[43mlogits\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontiguous\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1215\u001b[0m     shift_labels \u001b[38;5;241m=\u001b[39m labels[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;241m1\u001b[39m:]\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[1;32m   1216\u001b[0m     \u001b[38;5;66;03m# Flatten the tokens\u001b[39;00m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 4.40 GiB. GPU 0 has a total capacity of 22.07 GiB of which 3.42 GiB is free. Including non-PyTorch memory, this process has 18.64 GiB memory in use. Of the allocated memory 18.19 GiB is allocated by PyTorch, and 164.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# start training, the model will be automatically saved to the hub and the output directory\n",
    "trainer.train()\n",
    " \n",
    "# save model\n",
    "trainer.save_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (System)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
